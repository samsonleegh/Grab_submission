{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "import Geohash as geo\n",
    "import matplotlib.ticker as plticker\n",
    "from datetime import timedelta\n",
    "\n",
    "## for preprocessing and machine learning\n",
    "from sklearn.cluster import KMeans, k_means\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sklearn.linear_model as linear_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "## for Deep-learing:\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Behavioural Patterns before forecast\n",
    "To acquire good features for forecasting, we first have to understand different demand behaviour by the population across time and space. <br>\n",
    "People demand for transport differently throughout the time of day (going to work and coming home), the day of the week (weekdays vs weekends). <br>\n",
    "Demand behaviours are also different from city centers, auxiliary towns and more rural regions. For example, demand could be higher in the morning when people travel from auxiliary towns into city centers and vice veras in the evenings. <br>\n",
    "\n",
    "The behavioural analysis will first look into different time trends (across the entire 60 days, time of the day and day of the week). <br>\n",
    "Next we will do some clustering analysis to understand demand patterns from different regions and how they interact with time trends. <br>\n",
    "\n",
    "Throughout the analysis, complementary features will be added for the forecasting. <br>\n",
    "The forecasting will be conducted largely in three parts.<br>\n",
    "1. A time series aggregated demand forecast for each 15 minute intervals at clustered regions.\n",
    "2. This is followed by a distribution or proportionate forecast of the aggregated demand across individual geolocations in each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() #no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.distplot(df['demand'], label='demand')\n",
    "\n",
    "#demand follows a lognormal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,6))\n",
    "dd_ts = df.groupby('day')['demand'].sum().reset_index()\n",
    "sns.lineplot(x='day', y='demand', data=dd_ts, ax=ax)\n",
    "plt.show()\n",
    "#day trend shows demand growing with cyclical effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Create a dummy datetime using 2019 1st Jan as startdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "df['day_time'] = df['day'].astype('str') +':'+ df['timestamp']\n",
    "df['day_time2'] = df['day_time'].apply(lambda x: x.split(':'))\n",
    "df['day_time3'] = df['day_time2'].apply(lambda x: timedelta(days=int(x[0]),hours=int(x[1]),minutes=int(x[2])))\n",
    "df['dum_time'] = pd.Timestamp(2019,1,1).normalize() + df['day_time3']\n",
    "df.drop(['day_time','day_time2','day_time3'],axis=1,inplace=True) # drop irrelevant columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'] = df['dum_time'].dt.time\n",
    "df['hour'] = df['dum_time'].dt.hour\n",
    "df['minute'] = df['dum_time'].dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "dd_ts = df.groupby('hour')['demand'].sum().reset_index()\n",
    "sns.lineplot(x='hour', y='demand', data=dd_ts, ax=ax)\n",
    "loc = plticker.MultipleLocator() # this locator puts ticks at regular intervals\n",
    "ax.xaxis.set_major_locator(loc)\n",
    "plt.show()\n",
    "#time trend shows peak in the morning and trough at 1900-2000hrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create day cycles of 7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daycycle_dict1 = {}\n",
    "for i in range(1,8):\n",
    "    j = i\n",
    "    while j <= 61:\n",
    "        daycycle_dict1[j] = i\n",
    "        j+=7\n",
    "df['daycycle'] = df['day'].apply(lambda x: daycycle_dict1[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "dd_ts = df.groupby('daycycle')['demand'].sum().reset_index()\n",
    "sns.lineplot(x='daycycle', y='demand', data=dd_ts, ax=ax)\n",
    "plt.show()\n",
    "#Each day of the week has different demand. The 5th & 6th days could be the weekends due to the change in demand behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['geohash6'].value_counts().sort_values().head()\n",
    "#there are locations with 1 or little datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('geohash6').agg(\n",
    "    {\n",
    "         'demand':\"median\",    # median demand for each location\n",
    "         'geohash6': \"count\",  # no. of datapoints for each location\n",
    "    }\n",
    ").plot(x=\"geohash6\", y=\"demand\", kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low demand areas indeed has less datapoints. <br>\n",
    "Assume the missing datapoint is due to zero demand at location for the particular day time. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get lat & long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['latitude'] = df['geohash6'].apply(lambda x: geo.decode_exactly(x)[0])\n",
    "df['longitude'] = df['geohash6'].apply(lambda x: geo.decode_exactly(x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "dd_loc = df.groupby(['latitude','longitude'])['demand'].sum().reset_index()\n",
    "sns.scatterplot(x='longitude', y='latitude', size='demand', sizes=(40, 400), data=dd_loc, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupby(['geohash6','latitude','longitude'])['demand'].agg(['sum','std']).fillna(0).reset_index() #treat those with nan standard deviation as 0\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find location clusters based on proximity and demand indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.drop('geohash6',axis=1)\n",
    "Xs  = StandardScaler().fit_transform(X)\n",
    "Xs  = pd.DataFrame(Xs , columns = X.columns.values)\n",
    "Xs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_clusters(X, scaling=StandardScaler, k=11):\n",
    "    #choosing clusters with elbow within cluster sum square errors and silhouette score\n",
    "    inertia = []\n",
    "    silh = []\n",
    "    #standardizing required\n",
    "    Xs = StandardScaler().fit_transform(X)\n",
    "    Xs = pd.DataFrame(Xs, columns = X.columns.values)\n",
    "    for i in range(1,k):\n",
    "        model = KMeans(n_clusters=i, random_state=0).fit(Xs)\n",
    "        predicted = model.labels_\n",
    "        inertia.append(model.inertia_)#low inertia = low cluster sum square error. Low inertia -> Clusters are more compact.\n",
    "        if i>1:\n",
    "            silh.append(silhouette_score(Xs, predicted, metric='euclidean')) #High silhouette score = clusters are well separated. The score is based on how much closer data points are to their own clusters (intra-dist) than to the nearest neighbor cluster (inter-dist): (cohesion + separation).  \n",
    "    plt.plot(np.arange(1, k, step=1), inertia)\n",
    "    plt.title('Innertia vs clusters')\n",
    "    plt.xlabel('No. of clusters')\n",
    "    plt.ylabel('Within Clusters Sum-sq (WCSS)')\n",
    "    plt.show()\n",
    "    plt.scatter(np.arange(2, k, step=1), silh)\n",
    "    plt.title('Sihouette vs clusters')\n",
    "    plt.xlabel('No. of clusters')\n",
    "    plt.ylabel('Silhouette score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_clusters(Xs, scaling=StandardScaler, k=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting prediction and centroids\n",
    "#select 6 clusters based on silhouette and WCSS\n",
    "kmeans = KMeans(n_clusters=6, random_state=0).fit(Xs)\n",
    "predicted = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "Xs['predicted'] = predicted #or X['predicted'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['cluster'] = Xs['predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "sns.scatterplot(x='longitude', y='latitude', size='sum', hue='cluster', palette=sns.color_palette(\"Dark2\", 6), sizes=(40, 400), data=df2, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looks like the k-mean models has clustered 3 main type of locations\n",
    "1. 1st tier size clusters which probably are the centres of activities\n",
    "2. 2nd tier size clusters which probably are the busier areas nearby centres\n",
    "3. Areas with lower activities, seperated into NE NW SE SW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary for these locations\n",
    "cluster_dict = df2[['geohash6','cluster']].set_index('geohash6')['cluster'].to_dict()\n",
    "df['cluster'] = df['geohash6'].apply(lambda x: cluster_dict[x])\n",
    "loc_dict = {0:'clust0', 1:'clust1', 2:'clust2', 3:'clust3', 4:'clust4', 5:'clust5'}\n",
    "df['cluster'] = df['cluster'].apply(lambda x: loc_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tried VAR Time series forecast. Results were not encouraging. Use LSTM to predict cluster demand for t+1,t+2,...,t+5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing to create cluster time series dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping and pivoting each cluster's demand\n",
    "lstm_df = df.groupby(['dum_time','cluster','daycycle','hour','minute'])['demand'].sum().reset_index()\n",
    "lstm_df.set_index('dum_time', inplace=True)\n",
    "\n",
    "lstm_df2 = pd.pivot_table(lstm_df, values = 'demand', index=['dum_time','daycycle','hour','minute'], columns = 'cluster').reset_index()\n",
    "lstm_df2.set_index('dum_time',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing demand min max for better processing of neural nets\n",
    "series = lstm_df2.drop(['daycycle','hour','minute'],axis=1)\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "scaled = scaler.fit_transform(series.values)\n",
    "series_ss = pd.DataFrame(scaled)\n",
    "\n",
    "series_ss.columns = list(series.columns)\n",
    "series_ss.set_index(series.index, inplace=True)\n",
    "series_ss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dataset for each time series forecast t+1, t+2, t+3, t+4, t+5\n",
    "tseries_dict = {}\n",
    "for step in range(1,6):\n",
    "    y = series_ss.copy()\n",
    "    ts_prior = series_ss.shift(step)\n",
    "    y.columns = [j + str(step) for j in list(series_ss.columns)]\n",
    "    dummies = pd.get_dummies(lstm_df2[['daycycle','hour','minute']], columns = ['daycycle','hour','minute'],drop_first=True)\n",
    "    tseries_dict[step] = pd.concat([ts_prior,dummies,y],axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### function to split into train and test sets\n",
    "def train_test_prep(data):\n",
    "    values = data.values\n",
    "    n_train_time = round(0.9*len(data))\n",
    "    train = values[:n_train_time, :]\n",
    "    test = values[n_train_time:, :]\n",
    "    ##test = values[n_train_time:n_test_time, :]\n",
    "    # split into input and outputs\n",
    "    train_X, train_y = train[:, :-6], train[:, -6:]\n",
    "    test_X, test_y = test[:, :-6], test[:, -6:]\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "    train_X.shape, train_y.shape, test_X.shape, test_y.shape\n",
    "    return train_X, train_y, test_X, test_y\n",
    "# We reshaped the input into the 3D format as expected by LSTMs, namely [samples, timesteps, features]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(round(0.9*len(tseries_dict[1])))\n",
    "# tseries_dict[1].iloc[5256,:]\n",
    "# date where training set starts is 2019-02-25 22:00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train/test X&y datasets for each step forecast\n",
    "Xts_train = {}\n",
    "Xts_test = {}\n",
    "yts_train = {}\n",
    "yts_test = {}\n",
    "for step in range(1,6):\n",
    "    Xts_train[step], yts_train[step], Xts_test[step], yts_test[step] = train_test_prep(tseries_dict[step])\n",
    "\n",
    "tseries_dict[step].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training model\n",
    "def model_train(train_X,train_y,test_X,test_y):\n",
    "    model = Sequential()\n",
    "    model.reset_states()\n",
    "    model.add(LSTM(input_shape=(train_X.shape[1], train_X.shape[2]), output_dim=50, return_sequences = True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(6))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')    \n",
    "    history = model.fit(train_X, train_y, epochs=20, batch_size=100, validation_data=(test_X, test_y), verbose=0, shuffle=False)        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and save models forecasting t+1,t+2,...,t+5\n",
    "models_dict = {}\n",
    "\n",
    "for step in range(1,6):\n",
    "    model = model_train(Xts_train[step], yts_train[step], Xts_test[step], yts_test[step])\n",
    "    model.save('timestep_'+str(i)+'.h5')\n",
    "    models_dict[step] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get error and forecast plot on test set\n",
    "def model_predict(model,test_X,test_y):\n",
    "    # make a prediction\n",
    "    yhat = model.predict(test_X)\n",
    "    # invert scaling for forecast\n",
    "    yhat = yhat.reshape((len(test_y), 6))\n",
    "    inv_yhat = scaler.inverse_transform(yhat)\n",
    "\n",
    "    # invert scaling for actual\n",
    "    test_y = test_y.reshape((len(test_y), 6))\n",
    "    inv_y = scaler.inverse_transform(test_y)\n",
    "\n",
    "    rmse_ls = []\n",
    "    for i in range(0,6):\n",
    "        rmse_ls.append(mean_squared_error(pd.DataFrame(inv_y)[i], pd.DataFrame(inv_yhat)[i]))\n",
    "\n",
    "    mean_val = pd.DataFrame(inv_yhat).mean()\n",
    "    error_df = pd.concat([pd.DataFrame(rmse_ls, columns=['rmse']),mean_val],axis=1)\n",
    "    error_df['%_error'] = error_df['rmse']/error_df[0]*100\n",
    "    error_df.columns = ['rmse','mean_val','%_error']\n",
    "    print(error_df)\n",
    "    \n",
    "#     for i in range(0,6):\n",
    "#         print(series.columns[i])\n",
    "#         plt.plot(inv_y[:,i], marker='.', label=\"actual\")\n",
    "#         plt.plot(inv_yhat[:,i], 'r', label=\"prediction\")\n",
    "#         plt.legend(fontsize=10)\n",
    "#         plt.show()\n",
    "    \n",
    "    fig, ax = plt.subplots(2, 3, figsize=(20,7))\n",
    "    ax[0, 0].plot(inv_y[:,0], marker='.', label=\"actual\")\n",
    "    ax[0, 1].plot(inv_y[:,1], marker='.', label=\"actual\")\n",
    "    ax[0, 2].plot(inv_y[:,2], marker='.', label=\"actual\")\n",
    "    ax[1, 0].plot(inv_y[:,3], marker='.', label=\"actual\")\n",
    "    ax[1, 1].plot(inv_y[:,4], marker='.', label=\"actual\")\n",
    "    ax[1, 2].plot(inv_y[:,5], marker='.', label=\"actual\")\n",
    "    ax[0, 0].plot(inv_yhat[:,0], marker='.', label=\"prediction\")\n",
    "    ax[0, 1].plot(inv_yhat[:,1], marker='.', label=\"prediction\")\n",
    "    ax[0, 2].plot(inv_yhat[:,2], marker='.', label=\"prediction\")\n",
    "    ax[1, 0].plot(inv_yhat[:,3], marker='.', label=\"prediction\")\n",
    "    ax[1, 1].plot(inv_yhat[:,4], marker='.', label=\"prediction\")\n",
    "    ax[1, 2].plot(inv_yhat[:,5], marker='.', label=\"prediction\")\n",
    "    ax[0, 0].title.set_text('Cluster0')\n",
    "    ax[0, 1].title.set_text('Cluster1')\n",
    "    ax[0, 2].title.set_text('Cluster2')\n",
    "    ax[1, 0].title.set_text('Cluster3')\n",
    "    ax[1, 1].title.set_text('Cluster4')\n",
    "    ax[1, 2].title.set_text('Cluster5')\n",
    "    ax[0, 0].legend(fontsize=10,loc='upper right')\n",
    "    plt.show()\n",
    "    return inv_yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save predicted cluster demand for each cluster for t+1,..t+5 into a data dictionary\n",
    "y_pred = {}\n",
    "for step in range(1,6):\n",
    "    print('Forecast time ahead ', step)\n",
    "    y_pred[step] = model_predict(models_dict[step],Xts_test[step], yts_test[step])\n",
    "\n",
    "#error increase as timestep ahead to predict increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Cross Sector Analysis\n",
    "This analysis is to tease out the relationship of geolocation demand proportion to its overall cluster demand. <br>\n",
    "The relationship is teased out at the level of geolocation's daycycle, hour & 15minute intervals.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize lat and long for analysis later\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "lat_long = MinMaxScaler(feature_range=(0, 1), copy=True).fit_transform(df[['latitude','longitude']])\n",
    "lat_long = pd.DataFrame(lat_long)\n",
    "lat_long.columns = ['latitude','longitude']\n",
    "\n",
    "df['latitude'] = lat_long['latitude']\n",
    "df['longitude'] = lat_long['longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get proportion of demand for each geolocation, relative to the cluster and time\n",
    "sector_dd = df.groupby(['cluster','dum_time'])['demand'].sum().reset_index()\n",
    "df_sect_dd = df.merge(sector_dd, left_on = ['cluster','dum_time'], right_on = ['cluster','dum_time'],how = 'inner',suffixes=['','_sect'])\n",
    "df_sect_dd['prop_dd'] = df_sect_dd['demand']/df_sect_dd['demand_sect']\n",
    "df_sect_dd['prop_dd'] = df_sect_dd['prop_dd'].fillna(0)\n",
    "df_sect_dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get required data for cross section analysis\n",
    "df3 = df_sect_dd[['dum_time','geohash6','latitude','longitude','daycycle','cluster','hour','minute','demand','demand_sect','prop_dd']]\n",
    "#training set will be prior to '2019-02-25 22:00:00', as per time series forecast\n",
    "df3_train = df3[df3['dum_time'] < pd.Timestamp(2019,2,25,22,0)].drop(['dum_time'],axis=1)\n",
    "df3_test = df3[df3['dum_time'] >= pd.Timestamp(2019,2,25,22,0)].drop(['dum_time'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineer for train set\n",
    "#insert train set demand & prop_dd further statistics - mean,median,std,min,max\n",
    "\n",
    "f = {'demand': ['median','std','mean','min','max'],'prop_dd': ['median','std','mean','min','max']}\n",
    "df3_train2 = df3_train.groupby(['geohash6','cluster','daycycle','hour','minute']).agg(f).reset_index()\n",
    "df3_train2.columns = [\"\".join(x) for x in df3_train2.columns.ravel()]\n",
    "df3_train2 = df3_train2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check consistency/fluctuations of location proportion dd for each day cycle's time interval\n",
    "print('median prop_dd:', np.log(df3_train2['prop_ddstd'].median()))\n",
    "sns.distplot(np.log(df3_train2[df3_train2['prop_ddstd']!=0]['prop_ddstd']))\n",
    "plt.show()\n",
    "#fluctuation of prop_dd has a lognormal distribution but a heavier left tail. Most geolocation has low fluctuations in proportion demand, consistency is present / low data available for these geolocations as well.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as we are predicting prop_dd (before getting actual demand from multiplying cluster forecast), \n",
    "#remove demand from the data set and use prop_dd as target variable. \n",
    "df3_train_feat = df3_train.drop(['demand'],axis=1).merge(df3_train2, left_on = ['geohash6','cluster','daycycle','hour','minute'], right_on = ['geohash6','cluster','daycycle','hour','minute'],how = 'inner',suffixes=['','_feat'])\n",
    "df3_test_feat = df3_test.drop(['demand'],axis=1).merge(df3_train2, left_on = ['geohash6','cluster','daycycle','hour','minute'], right_on = ['geohash6','cluster','daycycle','hour','minute'],how = 'inner',suffixes=['','_feat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_train_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "#dummy obj variables\n",
    "#dont use geohash6 as dummy, giving too sparse matrix. use lat and longitude instead, under continuous variables\n",
    "X_train = pd.get_dummies(df3_train_feat,columns=['cluster','daycycle','hour','minute'],drop_first=True).drop(['prop_dd','geohash6'],axis=1).values\n",
    "y_train = df3_train_feat['prop_dd'].values\n",
    "\n",
    "X_test = pd.get_dummies(df3_test_feat,columns=['cluster','daycycle','hour','minute'],drop_first=True).drop(['prop_dd','geohash6'],axis=1).values\n",
    "y_test = df3_test_feat['prop_dd'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use basic linear regression. Have tried various other models - ridge, lasso, neural nets. Performance slightly better but with much longer processing time. Not worth it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "#fit model\n",
    "model = lm.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('In-sample R-sq:',model.score(X_train, y_train))\n",
    "print('Out-sample R-sq:',model.score(X_test, y_test))\n",
    "print('MSE:', mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The X predictors could explain about 64% of the target (proportion of demand) variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = y_test - predictions\n",
    "plt.scatter(resid, y_test)\n",
    "#there are still unobserved linear relationship present, must further tease out in next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ridge and lasso regressions\n",
    "# from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# #Ridge\n",
    "# #selecting optimal alpha/lambda\n",
    "# ridge_alphas = np.logspace(-2, 7, 20)\n",
    "# optimal_ridge = RidgeCV(alphas=ridge_alphas, cv=3)\n",
    "# optimal_ridge.fit(X_train, y_train)\n",
    "# print (optimal_ridge.alpha_)\n",
    "\n",
    "# #fit model\n",
    "# ridge = Ridge(alpha=optimal_ridge.alpha_)\n",
    "# model = ridge.fit(X_train, y_train)\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# print('In-sample R-sq:',model.score(X_train, y_train))\n",
    "# print('Out-sample R-sq:',model.score(X_test, y_test))\n",
    "# print('MSE:', mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural net\n",
    "# #train/val split\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train_part, X_val, y_train_part, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# modelse = models.Sequential()\n",
    "# modelse.add(layers.Dense(128, activation='relu', input_shape=(X_train_part.shape[1],))) # alternative input_shape: x_train.shape[1]; param 10000*16+16; \n",
    "# modelse.add(Dropout(0.5))\n",
    "# modelse.add(layers.Dense(256, activation='relu', input_shape=(128,))) #param 16*16+16\n",
    "# modelse.add(Dropout(0.5))\n",
    "# modelse.add(layers.Dense(64, input_shape=(256,)))\n",
    "# modelse.add(Dense(1, activation='sigmoid'))\n",
    "# modelse.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# modelse.summary()\n",
    "\n",
    "# # fit network\n",
    "# history = modelse.fit(X_train_part, y_train_part, epochs=15, batch_size=1000, validation_data=(X_val, y_val), verbose=1, shuffle=True)\n",
    "\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper right')\n",
    "# plt.show()\n",
    "\n",
    "# predictions = modelse.predict(X_test)\n",
    "# print(model.score(X_test, y_test))\n",
    "# print(mean_squared_error(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
